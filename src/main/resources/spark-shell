#!/bin/bash

SCRIPTS="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
TARGET="$( cd "$SCRIPTS/../target" && pwd )"

source "$SCRIPTS/default-spark-env"

source "$SCRIPTS/options.bash"

CONF_KRYO="spark.serializer=org.apache.spark.serializer.KryoSerializer"
if [ -n "${kryo.registrator.class}" ]; then CONF_KRYO="spark.kryo.registrator=${kryo.registrator.class}"; fi
$SPARK_HOME/bin/spark-shell \
    --master yarn-client \
    --driver-memory $DRIVER \
    --num-executors $WORKERS \
    --executor-memory $MEMORY \
    --executor-cores 1 \
    --queue "prod" \
    --name "spark-hbase-demo-terminal" \
    --conf "spark.app.name=spark-hbase-demo-terminal" \
    --conf "spark.yarn.jar=$SPARK_REMOTE_JAR" \
    --conf "spark.akka.frameSize=300" \
    --conf "spark.serializer=org.apache.spark.serializer.KryoSerializer" \
    --conf "$CONF_KRYO" \
    --conf "spark.kryo.referenceTracking=false" \
    --conf "spark.yarn.executor.memoryOverhead=$OVERHEAD" \
    --conf "spark.storage.memoryFraction=0.4" \
    --conf "spark.shuffle.memoryFraction=0.3" \
    --conf "spark.shuffle.spill=true" \
    --conf "spark.shuffle.manager=sort" \
    --conf "spark.sql.shuffle.partitions=4096" \
    --conf "spark.hadoop.validateOutputSpecs=false" \
    --conf "spark.scheduler.minRegisteredResourcesRatio=1.0" \
    --conf "spark.scheduler.maxRegisteredResourcesWaitingTime=30000" \
    --conf "spark.executor.extraClassPath=$HBASE_HOME/*:$HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar:$HBASE_HOME/lib/guava-12.0.1.jar" \
    --conf "spark.executor.extraLibraryPath=$HADOOP_HOME/lib/native" \
    --conf "spark.executorEnv.LD_PRELOAD=/opt/jprofiler/librebind.so" \
    --conf "spark.executorEnv.REBIND_PORT=8849:0" \
    --conf "spark.executor.extraJavaOptions=-verbose:gc -XX:+UseSerialGC -XX:+UseCompressedOops -XX:+UseCompressedStrings $EXTRAS" \
    --conf "spark.driver.userClassPathFirst=true" \
    --conf "spark.driver.extraJavaOptions=-XX:PermSize=256M -XX:MaxPermSize=512M" \
    --conf "spark.executor.userClassPathFirst=true" \
    --conf "spark.executorEnv.HBASE_CONF_DIR=$HBASE_CONF_DIR" \
    --conf "spark.executorEnv.HADOOP_CONF_DIR=$HADOOP_CONF_DIR" \
    --driver-class-path "$HBASE_HOME/*:$HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar:$HBASE_HOME/lib/guava-12.0.1.jar" \
    --driver-library-path "$HADOOP_HOME/lib/native" \
    -i "$SCRIPTS/${shell.init.script}" \
    --jars "$TARGET/${main.jar}.jar"

# Dynamic allocation requires external shuffle service
#    --conf "spark.dynamicAllocation.enabled=true" \
#    --conf "spark.dynamicAllocation.minExecutors=64" \
#    --conf "spark.dynamicAllocation.maxExecutors=$WORKERS" \


